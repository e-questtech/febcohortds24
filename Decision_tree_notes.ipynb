{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphical representation of all the possible solutions to a decision based on certain conditions.\n",
    "\n",
    "Decisions are based on some conditions.\n",
    "\n",
    "Why Decision tree?\n",
    "They are easy to understand. \n",
    "\n",
    "\n",
    "If the size of a dataset is huge, then a single decision tree could lead to an overfit model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree terminology:\n",
    "    * Root node:a sample of the entire population\n",
    "    * Leaf node: a node that cannot be futher divided\n",
    "    * Pruning: removing unwanted sub nodes or branches of the tree\n",
    "    * Splitting: didviding a node into sub parts based on some condition\n",
    "    \n",
    "    \n",
    "    How does a decision Tree decide where to split?\n",
    "    Gini index: the measure of impurity. Measures the uncertainty at a single node.\n",
    "    Information gain:decrease in entropy after a dataset is split on the basis of an attribute. The attribute with the highest information gain is considered the best.\n",
    "    Reduction in variance is an algorithm used for continuous target variables\n",
    "    Chi square: an algorithm used to discover the difference between the parent node and the child nodes.\n",
    "    Impurity is the degree of randomness\n",
    "    \n",
    "Which attribute should be picked as the root node?\n",
    "-the attribute that best describes the data set, which is the node that gives the highest information gain.\n",
    "\n",
    "Entropy(E) = -P(yes)log2P(yes)-P(no)log2P(no)\n",
    "\n",
    "When P(yes)= P(no), Entropy(S) = 1\n",
    "\n",
    "Information gain = Entropy(S) - [Weighted average * Entropy(each feature)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm builds multiple decision trees and merges them. As a result of it being the combination of various decision trees, it gives a more accurate and stable prediction than decision tree algorithm.\n",
    "\n",
    "Random forests also corrects the tendency of overfitting in decision tree algorithms for large datasets.\n",
    "\n",
    "Random forests are trained using the bagging method. The bagging method is an ensemble learning method used to reduce variance in a dataset.\n",
    "Ensemble learning is a technique in machine learning which merges multiple models to build a more robust model.\n",
    "\n",
    "In the random forest algorithm, the dataset is divide into n samples. Each of these samples is trained with a decision tree. A vote is taken from each of the descision trees. The result with the majority of votes is picked as the randpm forsest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
